install.packages("AppliedPredictionModels")
install.packages("AppliedPredictionModeling")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training)
summary(training)
install.packages(hmisc)
install.packages("Hmisc")
install.packages("Hmisc")
library(Hmisc)
cutCement <- cut2(training$Cement, g=3)
plot(CompressiveStrength, colour=cutCement, data=training)
install.packages("ggplot2")
install.packages("ggplot2")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training) # 774 9
summary(training)
library(Hmisc)
cutCement <- cut2(training$Cement, g=3)
plot(CompressiveStrength, colour=cutCement, data=training)
install.packages("ggplot2")
update.packages(ask = FALSE)
system('defaults write org.R-project.R force.LANG en_US.UTF-8')
update.packages(ask = FALSE)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training) # 774 9
summary(training)
library(Hmisc)
cutCement <- cut2(training$Cement, g=3)
plot(CompressiveStrength, colour=cutCement, data=training)
old.packages()
install.packages("Hmisc")
old.packages()
defaults write org.R-project.R force.LANG en_US.UTF-8
System("defaults write org.R-project.R force.LANG en_US.UTF-8")
system('defaults write org.R-project.R force.LANG en_US.UTF-8')
install.packages("AppliedPredictiveModeling")
install.packages("ElementStatLearn")
install.packages("ElemStatLearn")
install.packages("pgmm")
install.packages("rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret")
install.packages("caret")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret", dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
dim(segmentationOriginal)
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(segmentationOriginal$Case, p = 0.70,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing <- segmentationOriginal[-trainIndex,]
set.seed(125)
modFit <- train(Case ~ ., method = "rpart", data = training)
plot(modFit$finalModel, uniform=TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = 0.8)
install.packages("rattle")
install.packages("rattle", dependencies = TRUE)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("RGtk2")
install.packages("rattle")
install.packages("rattle", dependencies = TRUE, type = "source")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("magrittr")
install.packages("magrittr")
install.packages("magrittr")
install.packages("RGtk2")
install.packages("rattle", dependencies=TRUE, repos='http://cran.rstudio.com/')
install.packages("rattle", dependencies=TRUE, repos='http://cran.rstudio.com/')
library(rattle)
version
library(pgmm)
data(olive)
library(pgmm)
data(olive)
olive = olive[,-1]
dim(olive)
summary(olive)
newdata = as.data.frame(t(colMeans(olive)))
head(newdata)
install.packages("rpart.plot")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing <- segmentationOriginal[-trainIndex,]
modFit <- train(Case ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing <- segmentationOriginal[-trainIndex,]
modFit <- train(Case ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
library(pgmm)
data(olive)
library(caret)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- olive[trainIndex,]
testing <- olive[-trainIndex,]
modFit <- train(Case ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
question3 <- funtion() {
library(pgmm)
data(olive)
library(caret)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- olive[trainIndex,]
testing <- olive[-trainIndex,]
modFit <- train(Area ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
}
library(pgmm)
data(olive)
library(caret)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- olive[trainIndex,]
testing <- olive[-trainIndex,]
modFit <- train(Area ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
summary(olive)
n4 <- function() {
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
}
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(SAheart)
summary(SAheart)
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial")
prediction <- predict(modFit, newdata = testSA)
values <- testSA[, c("chd")]
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data = trainSA)
prediction <- predict(modFit, newdata = testSA)
values <- testSA[, c("chd")]
Summary(trainSA)
Summary(train)
summary(trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass()
missClass(values, prediction)
trainPrediction <- predict(modFit, newdata = trainSA)
trainValues <- trainSA$chd
missClass(trainValues, trainPrediction)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
model <- randomForest(y ~ ., ntree, data = vowel.train)
library(randomForest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
model <- randomForest(y ~ ., ntree, data = vowel.train)
model <- randomForest(y ~ ., data = vowel.train)
varImp(model)
sorted(varImp(model))
data(vowel.train)
library(ElemStatLearn)
data(vowel.train)
summary(vowel.train)
setwd("~/Desktop/ML")
source("Quiz4.R")
question1()
source("Quiz4.R")
question1()
source("Quiz4.R")
question1()
source("Quiz4.R")
question1()
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
set.seed(33833)
modelrf <- train(y ~ ., data = vowel.train, method = "rf")
modelgbm <- train(y ~ ., data = vowel.train, method = "gbm")
predictionRf <- predict(modelrf, newdata = vowel.test)
predictionGbm <- predict(modelgbm, newdata = vowel.test)
predDF <- data.frame(predictionRf, predictionGbm, y = vowel.test$y)
combModel <- train(y ~ ., method="gam", data = predDF)
combPred <- predict(combModel, predDF)
errores <- c(sqrt(sum((predictionRf - vowel.test$y)^2)),
sqrt(sum((predictionGbm - vowel.test$y)^2)),
sqrt(sum((combPred - vowel.test$y)^2)))
errores
confusionMatrix(predictionRf)
confusionMatrix(vowel.test$y, predictionRf)
summary(vowel.test$y)
summary(predictionRf)
dim(predictionRf)
len(predictionRf)
lenght(predictionRf)
length(predictionRf)
length(vowel.test$y)
head(y)
head(vowel.test$y)
predictionRf <- round(predictionRf)
predictionGbm <- round(predictionRf)
confusionMatrix(vowel.test$y, predictionRf)
head(predictionRf)
head(vowel.test$y)
confusionMatrix(round(vowel.test$y), predictionRf)
head(predictionRf, 20)
x <- as.data.frame(predictionRf)
x
head(x)
y <- as.data.frame(vowel.test$y)
confusionMatrix(y, x)
x <- as.list(x)
y <- as.list(y)
confusionMatrix(y, x)
head(x)
table(y, x)
x <- unlist(x)
y <- unlist(y)
confusionMatrix(y, x)
x <- as.data.frame(x)
y <- as.data.frame(y)
confusionMatrix(y, x)
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
modelrf <- train(y ~ ., data = vowel.train, method = "rf")
modelgbm <- train(y ~ ., data = vowel.train, method = "gbm")
predictionRf <- round(predict(modelrf, newdata = vowel.test))
predictionGbm <- round(predict(modelgbm, newdata = vowel.test))
predDF <- data.frame(predictionRf, predictionGbm, y = vowel.test$y)
confusionMatrix(predictionRf, vowel.test$y)$overall[1]
confusionMatrix(predictionGbm, vowel.test$y)$overall[1]
predictionRf <- predict(modelrf, newdata = vowel.test)
predictionGbm <- predict(modelgbm, newdata = vowel.test)
confusionMatrix(predictionRf, vowel.test$y)$overall[1]
confusionMatrix(predictionGbm, vowel.test$y)$overall[1]
combPred <- sum(predictionGbm[predDF$predictionRf == predDF$predictionGbm] ==
predDF$y[predDF$predictionRf == predDF$predictionGbm]) /
sum(predDF$predictionRf == predDF$predictionGbm)
confusionMatrix(combPred, vowel.test$y)$overall[1]
head(combPred)
combPred <- sum(predDF[predDF$predictionRf == predDF$predictionGbm] ==
predDF$y[predDF$predictionRf == predDF$predictionGbm]) /
sum(predDF$predictionRf == predDF$predictionGbm)
combPred <- sum(predictionRf[predDF$predictionRf == predDF$predictionGbm] ==
predDF$y[predDF$predictionRf == predDF$predictionGbm]) /
sum(predDF$predictionRf == predDF$predictionGbm)
combPred
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modelRf <- train(diagnosis ~ ., data = training, method = "rf")
modelGbm <- train(diagnosis ~ ., data = training, method = "gbm")
modelLda <- train(diagnosis ~ ., data = training, method = "lda")
predictionRf <- predict(modelrf, newdata = testing)
predictionGbm <- predict(modelgbm, newdata = testing)
predictionLda <- predict(modelrf, newdata = testing)
confusionMatrix(predictionRf, testing$diagnosis)$overall[1]
confusionMatrix(predictionGbm, testing$diagnosis)$overall[1]
confusionMatrix(predictionLda, testing$diagnosis)$overall[1]
predictionRf <- predict(modelRf, newdata = testing)
predictionGbm <- predict(modelGbm, newdata = testing)
predictionLda <- predict(modelLda, newdata = testing)
confusionMatrix(predictionRf, testing$diagnosis)$overall[1]
confusionMatrix(predictionGbm, testing$diagnosis)$overall[1]
confusionMatrix(predictionLda, testing$diagnosis)$overall[1]
predDF <- data.frame(predictionRf, predictionGbm, predictionLda, diagnosis = testing$diagnosis)
modelComb <- train(diagnosis~., data=combined.data, method="rf")
predictionComb <- predict(combined.model, testing)
confusionMatrix(predictionComb, testing$diagnosis)$overall[1]
predDF <- data.frame(predictionRf, predictionGbm, predictionLda, diagnosis = testing$diagnosis)
modelComb <- train(diagnosis~., data=predDF, method="rf")
predictionComb <- predict(combined.model, testing)
confusionMatrix(predictionComb, testing$diagnosis)$overall[1]
predDF <- data.frame(predictionRf, predictionGbm, predictionLda, diagnosis = testing$diagnosis)
modelComb <- train(diagnosis~., data=predDF, method="rf")
predictionComb <- predict(modelComb, testing)
confusionMatrix(predictionComb, testing$diagnosis)$overall[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
model <- train(CompressiveStrength ~ ., data = training, method = "lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
model <- train(CompressiveStrength ~ ., data = training, method = "lasso")
plot.enet(model$finalModel,xvar="penalty", use.color=TRUE)
library(lubridate) # For year() function below
dat = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"))
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
nTestinRows <- nrow(testing)
model <- bats(tstrain)
fcast <- forecast(model, level = 95, nTestinRows)
result <- sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper)/nTestinRows
install.packages("lubridate")
library(lubridate) # For year() function below
dat = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"))
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
nTestinRows <- nrow(testing)
model <- bats(tstrain)
fcast <- forecast(model, level = 95, nTestinRows)
result <- sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper)/nTestinRows
install.packages("forecast")
library(lubridate) # For year() function below
dat = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"))
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
nTestinRows <- nrow(testing)
model <- bats(tstrain)
fcast <- forecast(model, level = 95, nTestinRows)
result <- sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper)/nTestinRows
install.packages("e1071")
install.packages("e1071")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model <- train(CompressiveStrength ~ ., data = training, method="svm")
predictionSvm <- predict(model, testing)
accuracy(predictionSvm, testing$CompressiveStrength)
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model <- train(CompressiveStrength ~ ., data = training, method="svm")
predictionSvm <- predict(model, testing)
accuracy(predictionSvm, testing$CompressiveStrength)
library(caret)
library("e1071")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model <- svm(CompressiveStrength ~ ., data = training)
predictionSvm <- predict(model, testing)
accuracy(predictionSvm, testing$CompressiveStrength)
rmse <- sqrt(sum((predictionSvm - testing$CompressiveStrength)^2))
rmse
set.seed(325)
model <- svm(CompressiveStrength ~ ., data = training)
predictionSvm <- predict(model, testing)
rmse <- sqrt(sum((predictionSvm - testing$CompressiveStrength)^2))
rmse
rmse <- sqrt(mean((predictionSvm - testing$CompressiveStrength)^2))
rmse
clear
clean
cls
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# Feature selection
# Removing variables with too much NA
na_threshold = 0.8
accepted_size <- nrow(df)*na_threshold
na_values_per_column <- colSums(is.na(df))
df <- df[, na_values_per_column > accepted_size]
dim(df)
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# Feature selection
# Removing variables with too much NA
na_threshold = 0.8
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column > accepted_size]
dim(raw_df)
dim(tidy_df)
na_threshold = 0.7
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column > accepted_size] # 100 variables
dim(tidy_df)
colnames(tidy_df)
colnames(raw_df)
colnames(tidy_df)
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size] # 100 variables
dim(tidy_df)
colnames(tidy_df)
summary(tidy_df)
colnames(tidy_df)
colnames(raw_df)
summary(tidy_df)
std(tidy_df)
sd(tidy_df)
x <- sapply(tidy_df, sd)
head(x)
sort(x)
hist(x$gyros_belt_y)
hist(x['gyros_belt_y'])
colnames(tidy_df)
library(caret)
# Spliting the data in training and testing set
set.seed(0328)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 10)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc)
